# ğŸ§  Hridaya â€” Emotional Wellness Assistant

> *"Hridaya" (à¤¹à¥ƒà¤¦à¤¯) â€” Sanskrit for "Heart"*

Hridaya is an AI-powered emotional wellness assistant that conducts structured, clinically-inspired therapy sessions through multimodal emotion detection. It combines a fine-tuned RoBERTa model, an audio emotion classifier, and Google's Gemma 3 LLM to understand how you truly feel â€” not just what you say.

---

## âœ¨ Key Features

- ğŸ™ï¸ **Multimodal Emotion Detection** â€” Fuses text analysis (RoBERTa + Gemma 3) with audio prosody analysis (MFCC/Chroma/Mel-spectrogram features) for accurate, conflict-aware emotion scoring
- ğŸ§‘â€âš•ï¸ **Structured 11-Turn Therapy Sessions** â€” Guided conversations with a clinically-aware AI therapist powered by Gemma 3 27B
- ğŸ“Š **Daily & Weekly Analytics Dashboard** â€” Interactive Streamlit dashboard with Plotly charts showing emotional trends, stability scores, and session breakdowns
- ğŸ¤– **AI-Generated Clinical Summaries** â€” Automatic extraction of stress triggers, happy moments, and wellness suggestions after each session
- ğŸ“„ **PDF Report Export** â€” Downloadable daily and weekly clinical PDF reports generated with ReportLab and a deep emotional profile from Gemma
- ğŸ—„ï¸ **ChromaDB Session Memory** â€” Completed sessions are persisted as semantic embeddings for historical RAG querying
- ğŸŒ **React + Vite Frontend** â€” A sleek dark-theme chat interface (Hridaya UI) with embedded Streamlit analytics dashboard via iframe

---

## ğŸ—ï¸ Project Architecture

```
Emotional_wellness_assistant/
â”‚
â”œâ”€â”€ backend_api_app.py          # FastAPI backend â€” core chat, emotion fusion, PDF generation
â”œâ”€â”€ emotional_dashboard.py      # Streamlit analytics dashboard (daily & weekly views)
â”‚
â”œâ”€â”€ production_emotion_model/   # Fine-tuned RoBERTa (text emotion classifier)
â”‚   â”œâ”€â”€ model.safetensors       # Model weights (~476 MB)
â”‚   â”œâ”€â”€ config.json             # Model config (7 emotion classes)
â”‚   â”œâ”€â”€ tokenizer.json          # Tokenizer vocabulary
â”‚   â””â”€â”€ tokenizer_config.json
â”‚
â”œâ”€â”€ audio_emotion_model.pkl     # Sklearn-based audio emotion classifier (~31 MB)
â”‚
â”œâ”€â”€ neural-ninjas-ui/           # React + Vite frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx             # Main UI: chat, sidebar nav, embedded dashboard
â”‚       â”œâ”€â”€ index.css           # Global styles
â”‚       â””â”€â”€ main.jsx            # Entry point
â”‚
â”œâ”€â”€ landing_page/               # Static HTML landing page with animations
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”‚
â”œâ”€â”€ patient_logs/               # Per-user session JSON logs (auto-created)
â”‚   â””â”€â”€ <username>/
â”‚       â””â”€â”€ Data_<user>_<date>_<timestamp>.json
â”‚
â”œâ”€â”€ rag_db/                     # ChromaDB vector store for session history
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ config.yml                  # Streamlit config
```

---

## ğŸ”„ How Hridaya Works â€” Data Flow

```
User (Browser)
     â”‚
     â”‚  Text + Optional Audio
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              React Frontend (Port 5173)              â”‚
â”‚  â€¢ 11-turn structured session UI                    â”‚
â”‚  â€¢ Voice recording (MediaRecorder + WebSpeechAPI)   â”‚
â”‚  â€¢ Embedded Streamlit dashboard (iframe, Port 8501) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚  POST /chat  (FormData: text + audio)
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend (Port 8000)             â”‚
â”‚                                                     â”‚
â”‚  TURN 1: Collect daily schedule                     â”‚
â”‚                                                     â”‚
â”‚  TURNS 2â€“11: Multimodal Emotion Fusion Pipeline     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. Context Resolution (Gemma 3)            â”‚    â”‚
â”‚  â”‚     Resolve pronouns, extract emotion scoresâ”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  2. Text Emotion Analysis (RoBERTa)         â”‚    â”‚
â”‚  â”‚     Fine-tuned on 7-class emotion dataset   â”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  3. Conflict Detection & Weight Assignment  â”‚    â”‚
â”‚  â”‚     If RoBERTa < 70% confidence OR conflict â”‚    â”‚
â”‚  â”‚     â†’ Trust Gemma more (80/20)              â”‚    â”‚
â”‚  â”‚     Else â†’ Trust RoBERTa more (70/30)       â”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  4. Audio Emotion Analysis (if audio sent)  â”‚    â”‚
â”‚  â”‚     MFCC + Chroma + Mel features â†’ sklearn  â”‚    â”‚
â”‚  â”‚     Final = 30% text fused + 70% audio      â”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  5. Detected Emotion â†’ Gemma 3 Therapist    â”‚    â”‚
â”‚  â”‚     Generates empathetic, contextual reply  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                     â”‚
â”‚  TURN 11 (Final):                                   â”‚
â”‚  â€¢ Generate Clinical Summary (triggers, joy, tips)  â”‚
â”‚  â€¢ Save JSON log to ./patient_logs/<username>/      â”‚
â”‚  â€¢ Persist session to ChromaDB                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                   â”‚
              â–¼                   â–¼
   patient_logs/<user>/       rag_db/
   Data_*.json               (ChromaDB)
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Streamlit Dashboard (Port 8501)             â”‚
â”‚  â€¢ Load JSON logs for selected user/date            â”‚
â”‚  â€¢ Daily View: Emotion pie, intensity line, flow    â”‚
â”‚  â€¢ Weekly View: Stacked bar, happiness pulse, AI    â”‚
â”‚    deep profile (Gemma), personalized weekly plan   â”‚
â”‚  â€¢ PDF export via /download-pdf and                 â”‚
â”‚    /download-weekly-pdf endpoints                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– The Emotion Detection System (Deep Dive)

Hridaya uses a **3-layer multimodal fusion** approach:

### Layer 1 â€” Context Resolution (Gemma 3 27B)
Before any analysis, the raw user message is sent to Gemma 3, which:
- Resolves pronouns and vague references using conversation history
- Provides its own emotion probability distribution across 7 classes
- This "smart preprocessing" handles sarcasm and ambiguous language

### Layer 2 â€” Text Emotion Classification (RoBERTa)
A fine-tuned `RobertaForSequenceClassification` model (`production_emotion_model/`) classifies the resolved text into 7 emotions:

| ID | Emotion   |
|----|-----------|
| 0  | Neutral   |
| 1  | Anger     |
| 2  | Disgust   |
| 3  | Fear      |
| 4  | Happiness |
| 5  | Sadness   |
| 6  | Surprise  |

### Layer 3 â€” Conflict-Aware Fusion
The system detects **emotional conflict** (e.g., RoBERTa says "happy" but Gemma 3 says "sad") and adapts weights dynamically:

```
If conflict detected OR RoBERTa confidence < 70%:
    final_text_score = 0.20 Ã— RoBERTa + 0.80 Ã— Gemma
Else:
    final_text_score = 0.70 Ã— RoBERTa + 0.30 Ã— Gemma
```

### Layer 4 â€” Audio Prosody (when mic is used)
If voice input is provided, audio features are extracted:
- **MFCCs** (40 coefficients) â€” captures tonal quality and speech patterns
- **Chroma** (12 features) â€” captures harmonic content
- **Mel Spectrogram** â€” captures frequency-time patterns

These are fed into a pre-trained sklearn classifier (`audio_emotion_model.pkl`), and the result overrides the text score:
```
final_score = 0.30 Ã— text_fused + 0.70 Ã— audio
```

---

## ğŸ—£ï¸ The 11-Turn Session Structure

Each session follows a structured clinical protocol:

| Turn | Phase | Purpose |
|------|-------|---------|
| 1 | Schedule Collection | User shares their daily schedule (no emotion analysis) |
| 2â€“6 | Positive Exploration | AI asks open-ended questions about good moments, highlights |
| 7â€“10 | Challenge Exploration | AI gently probes difficult or stressful aspects of the day |
| 11 | Session Closure | AI provides a 2-sentence summary, a wellness exercise, and a goal for tomorrow |

After Turn 11:
- A **Clinical Summary** is generated (triggers, happy moments, suggestions)
- The full session is **saved as JSON** and **embedded in ChromaDB**
- The user can **Talk More** (up to 5 extra turns) or **View Analytics**

---

## ğŸ–¥ï¸ Component Overview

### 1. `backend_api_app.py` â€” FastAPI Backend
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/chat` | POST | Main session endpoint â€” processes text/audio, returns AI reply + emotions |
| `/history/{username}` | GET | Returns list of session dates for a given user |
| `/download-pdf/{username}/{date}` | GET | Generates and returns a daily session PDF |
| `/download-weekly-pdf/{username}` | GET | Generates a full weekly wellness PDF with AI deep profile |

### 2. `emotional_dashboard.py` â€” Streamlit Analytics
- **Daily View**: Emotion distribution pie chart, intensity line chart, turn-by-turn emotional flow, AI insights cards, PDF download button
- **Weekly View**: Stacked mood bar chart, happiness pulse, emotional stability area chart, day-by-day breakdown, weekly highlights, Gemma-generated deep emotional health profile, personalized next-week plan

### 3. `neural-ninjas-ui/` â€” React Frontend
- Built with **Vite + React + TailwindCSS**
- Sidebar navigation between Daily Session, Daily Insights, and Weekly Analysis
- Full voice recording support (MediaRecorder API + Web Speech API for live transcript)
- Analytics dashboard is embedded as an iframe pointing to Streamlit (port 8501)
- Analytics are **locked** until a session is completed (privacy-first design)
- Native browser PDF print for session transcript export

### 4. `landing_page/` â€” Static Landing Page
- Animated HTML/CSS/JS landing page
- Login and Anonymous Login buttons
- Stores username in `localStorage` before routing to the chat UI

---

## âš™ï¸ Setup Guide

### Prerequisites

| Requirement | Version |
|-------------|---------|
| Python | 3.10+ |
| Node.js | 18+ |
| npm | 9+ |
| Google Gemma API Key | Required (Google AI Studio) |

> **Note:** The models (`production_emotion_model/` and `audio_emotion_model.pkl`) must be present locally. They are tracked via Git LFS due to their size (~530 MB total).

---

### Step 1 â€” Clone the Repository

```bash
git clone https://github.com/sinamchayan/Emotional_wellness_assistant.git
cd Emotional_wellness_assistant
```

---

### Step 2 â€” Set Up Python Virtual Environment

```bash
python3 -m venv .venv
source .venv/bin/activate       # Linux/macOS
# .venv\Scripts\activate        # Windows
```

---

### Step 3 â€” Install Python Dependencies

```bash
pip install -r requirements.txt
```

> âš ï¸ **Known Issue:** `openai-whisper` and `tensorflow`/`keras` may conflict. If you encounter dependency errors, install them in this order:
> ```bash
> pip install tensorflow
> pip install openai-whisper --no-deps
> ```

---

### Step 4 â€” Configure Your API Key

Open `backend_api_app.py` and `emotional_dashboard.py` and replace the API key:

```python
# In both files, find this line:
API_KEY = "your-google-ai-studio-key-here"
```

Get your free API key from [Google AI Studio](https://aistudio.google.com/).

---

### Step 5 â€” Install Frontend Dependencies

```bash
cd neural-ninjas-ui
npm install
cd ..
```

---

### Step 6 â€” Run All Services

You need **3 terminals** running simultaneously:

**Terminal 1 â€” FastAPI Backend:**
```bash
source .venv/bin/activate
cd /path/to/Emotional_wellness_assistant
uvicorn backend_api_app:app --host 0.0.0.0 --port 8000 --reload
```

**Terminal 2 â€” Streamlit Dashboard:**
```bash
source .venv/bin/activate
cd /path/to/Emotional_wellness_assistant
streamlit run emotional_dashboard.py --server.port 8501
```

**Terminal 3 â€” React Frontend:**
```bash
cd neural-ninjas-ui
npm run dev
```

---

### Step 7 â€” Access the App

| Service | URL |
|---------|-----|
| Landing Page | Open `landing_page/index.html` in a browser |
| Chat UI (React) | http://localhost:5173 |
| Analytics Dashboard (Streamlit) | http://localhost:8501 |
| API Docs (Swagger) | http://localhost:8000/docs |

---

## ğŸ“¦ Tech Stack

| Layer | Technology |
|-------|------------|
| LLM Backend | Google Gemma 3 27B (via Google GenAI SDK) |
| Text Emotion Model | Fine-tuned RoBERTa (`roberta-base`) |
| Audio Emotion Model | Sklearn classifier (MFCC/Chroma/Mel features via Librosa) |
| API Server | FastAPI + Uvicorn |
| Analytics Dashboard | Streamlit + Plotly |
| Session Storage | JSON files + ChromaDB (vector DB) |
| PDF Generation | ReportLab |
| Frontend | React 18 + Vite + TailwindCSS + Lucide Icons |

---

## ğŸ“ Patient Data & Privacy

- All session logs are stored **locally** in `./patient_logs/<username>/` as JSON files
- No data is sent to any external server except the **Google GenAI API** (for Gemma 3 inference)
- Session data is also stored in a local **ChromaDB** instance (`./rag_db/`) for semantic search
- The landing page supports **Anonymous Login** â€” no account or personal data required

---

## ğŸ§© Troubleshooting

| Problem | Solution |
|---------|----------|
| `Failed to fetch` on frontend | Ensure the FastAPI backend is running on port 8000 |
| ChromaDB SQLite FTS5 error | Run `python patch_db.py` to apply the SQLite patch |
| Streamlit analytics shows blank | Complete an 11-turn session first; analytics unlock after session ends |
| Weekly view locked | Requires **3 or more distinct session days** |
| Audio model not loading | Ensure `audio_emotion_model.pkl` is present in the root directory |
| RoBERTa model not loading | Ensure the `production_emotion_model/` folder contains `model.safetensors` |

---

## ğŸ™ Acknowledgements

- Model architecture based on **RoBERTa** by HuggingFace / Facebook AI
- Powered by **Google Gemma 3** via the Google GenAI SDK
- Audio feature extraction via **Librosa**
- Built with love by **Team Neural Ninjas** ğŸ§ âš¡

---

*"The mind is everything. What you think, you become."*
